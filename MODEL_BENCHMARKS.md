# Ollama モデルベンチマーク結果

## テスト環境
- **GPU**: NVIDIA GeForce RTX 5060 Ti (16GB)
- **CPU**: 12スレッド
- **RAM**: 27GB
- **テスト日**: 2025年8月10日

## ベンチマーク結果

### 応答速度テスト
*テストプロンプト: "What is 2+2?"*

| モデル | 初回ロード時間 | 応答時間 | GPU使用率 | メモリ使用 |
|--------|---------------|---------|-----------|-----------|
| llama3.2:latest | ~3秒 | <1秒 | 5-10% | 3.1 GiB |
| llama3.1:8b | ~5秒 | ~1秒 | 8-12% | 5.7 GiB |
| gpt-oss:gpu | ~8秒 | ~2秒 | 10-14% | 9.7 GiB |
| gpt-oss:20b | ~10秒 | ~5秒 | 0% (CPU) | RAM 14GB |
| phi3:mini | ~2秒 | <1秒 | 3-5% | 2.2 GiB |

### 長文生成テスト
*テストプロンプト: "Generate a 500 word essay about AI"*

| モデル | 生成時間 | トークン/秒 (推定) | 品質評価 |
|--------|---------|------------------|----------|
| llama3.2:latest | ~15秒 | ~35 tok/s | ★★★☆☆ |
| llama3.1:8b | ~20秒 | ~25 tok/s | ★★★★☆ |
| gpt-oss:gpu | ~30秒 | ~17 tok/s | ★★★★★ |
| gpt-oss:20b | ~60秒 | ~8 tok/s | ★★★★★ |

## モデル特性分析

### llama3.2:latest (2GB)
**長所:**
- 最速の応答速度
- 低メモリ使用量
- 完全GPU実行

**短所:**
- 生成品質が中程度
- 複雑なタスクには不向き

**推奨用途:**
- チャットボット
- 簡単な質問応答
- リアルタイム応答が必要な場面

### llama3.1:8b (4.9GB)
**長所:**
- バランスの良いパフォーマンス
- 高品質な生成
- 完全GPU実行

**短所:**
- 中程度のメモリ使用

**推奨用途:**
- 汎用的な言語タスク
- コード生成
- 文書要約

### gpt-oss:gpu (13GB, カスタム)
**長所:**
- 最高品質の生成
- 大規模モデルのGPU活用
- 思考プロセス表示

**短所:**
- 高メモリ使用
- ハイブリッド実行のオーバーヘッド

**推奨用途:**
- 複雑な推論タスク
- 高品質な文章生成
- プロフェッショナルな用途

### gemma2:27b (15GB)
**長所:**
- 超大規模モデル
- 最高レベルの理解力

**短所:**
- GPU非対応（メモリ不足）
- CPU実行で低速

**推奨用途:**
- 研究用途
- オフライン処理
- 最高精度が必要な場合

### phi3:mini (2.2GB)
**長所:**
- 軽量・高速
- 低リソース消費

**短所:**
- 限定的な能力

**推奨用途:**
- エッジデバイス
- 基本的なタスク
- リソース制限環境

## GPU最適化の効果

### gpt-oss:20b → gpt-oss:gpu 改善結果
```
実行環境:       CPU only → Hybrid (60% GPU)
応答速度:       5秒 → 2秒 (2.5倍高速化)
トークン/秒:    8 → 17 (2.1倍向上)
GPU使用:       0% → 10-14%
メモリ最適化:   14.9GB → 9.7GB (35%削減)
```

## パフォーマンスチューニング推奨事項

### 1. 用途別モデル選択ガイド

| 用途 | 第1選択 | 第2選択 | 理由 |
|-----|---------|---------|------|
| リアルタイムチャット | llama3.2 | phi3:mini | 高速応答 |
| コード生成 | llama3.1:8b | gpt-oss:gpu | バランス |
| 文書作成 | gpt-oss:gpu | llama3.1:8b | 品質重視 |
| 研究・分析 | gemma2:27b | gpt-oss:gpu | 精度重視 |
| エンベディング | mxbai-embed | - | 専用モデル |

### 2. コンテキスト長の影響

| コンテキスト長 | メモリ使用 | 推奨用途 |
|---------------|-----------|---------|
| 512 | 最小 | 短い会話 |
| 2048 | 標準 | 通常の会話 |
| 4096 | 高 | 長文処理 |
| 8192+ | 最大 | 文書分析 |

### 3. バッチサイズの影響

| バッチサイズ | スループット | レイテンシ |
|-------------|-------------|-----------|
| 32 | 低 | 最小 |
| 128 | 中 | 標準 |
| 512 | 高 | 最大 |

## 実測コマンド例

### 速度測定
```bash
# 応答時間測定
time echo "What is 2+2?" | ollama run llama3.2:latest

# トークン生成速度測定
echo "Write a story" | ollama run llama3.1:8b --verbose
```

### GPU使用率監視
```bash
# リアルタイム監視
watch -n 0.5 'nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv,noheader'

# 実行中のプロセス確認
nvidia-smi pmon -c 1
```

### メモリ使用量確認
```bash
# システムメモリ
free -h

# GPUメモリ
nvidia-smi --query-gpu=memory.used,memory.free --format=csv
```

## 結論

1. **小規模タスク**: llama3.2またはphi3:miniが最適
2. **汎用タスク**: llama3.1:8bが最良のバランス
3. **高品質要求**: gpt-oss:gpu（カスタム版）を使用
4. **GPU最適化**: 部分オフロードにより大規模モデルも実用的に

---
*ベンチマーク実施日: 2025年8月10日*